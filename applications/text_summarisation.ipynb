{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pegasus Text Summarization Tutorial\n",
    "\n",
    "Text summarisation application is done in the following steps:\n",
    "\n",
    "1. loading a pretrained model.\n",
    "2. initialize a tokenizer and the model.\n",
    "3. create an input text for testing.\n",
    "4. tokenize and use the pretrained model to summarise the input text.\n",
    "5. decode the generated token\n",
    "6. Finally, display the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vatha\\miniconda3\\envs\\llm-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the `llm-env` kernel before running the notebook\n",
    "\n",
    "**Note**\n",
    "install jupyter dependencies if needed before using this notebook\n",
    "install torch based on their official website either in CPU or GPU based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Pegasus Model and Tokenizer\n",
    "model_name = \"google/pegasus-xsum\"  # You can also use \"google/pegasus-large\" or other variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PegasusTokenizer requires the SentencePiece library. We can install using:\n",
    "`python -m pip install sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vatha\\miniconda3\\envs\\llm-env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vatha\\.cache\\huggingface\\hub\\models--google--pegasus-xsum. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "print(\"Tokenizer and model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Text for Summarization\n",
    "input_text = (\n",
    "    \"The COVID-19 pandemic has had a profound impact on global health and the economy. \"\n",
    "    \"Countries around the world have implemented measures to curb the spread of the virus, \"\n",
    "    \"including lockdowns, social distancing, and vaccination campaigns. Despite these efforts, \"\n",
    "    \"many challenges remain, including vaccine distribution, public compliance, and emerging variants.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: The COVID-19 pandemic has had a profound impact on global health and the economy. Countries around the world have implemented measures to curb the spread of the virus, including lockdowns, social distancing, and vaccination campaigns. Despite these efforts, many challenges remain, including vaccine distribution, public compliance, and emerging variants.\n"
     ]
    }
   ],
   "source": [
    "print(\"Input text:\", input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input text...\n",
      "Tokens: {'input_ids': tensor([[  139,  4585, 44078, 11545, 41428,   148,   196,   114,  9092,   979,\n",
      "           124,  1122,   426,   111,   109,  1968,   107, 23679,   279,   109,\n",
      "           278,   133,  4440,  2548,   112, 11762,   109,  2275,   113,   109,\n",
      "          5807,   108,   330, 59851,   116,   108,   525, 82585,   108,   111,\n",
      "         19138,  4515,   107,  4987,   219,  1645,   108,   223,  1628,  1686,\n",
      "           108,   330, 10733,  2807,   108,   481,  3529,   108,   111,  4610,\n",
      "         12565,   107,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Input Text\n",
    "print(\"Tokenizing input text...\")\n",
    "tokens = tokenizer(input_text, truncation=True, padding=\"longest\", max_length=512, return_tensors=\"pt\")\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary...\n",
      "Summary IDs: tensor([[    0,   139,   894,  1300,  7235,   143, 32916,   158,   148,  6130,\n",
      "           109,  4585, 11545, 41428,   204,   107,     1]])\n"
     ]
    }
   ],
   "source": [
    "# Generate Summary\n",
    "print(\"Generating summary...\")\n",
    "summary_ids = model.generate(tokens[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
    "print(\"Summary IDs:\", summary_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded summary: The World Health Organization (WHO) has declared the CO-19 pandemic over.\n"
     ]
    }
   ],
   "source": [
    "# Decode Summary\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(\"Decoded summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " The COVID-19 pandemic has had a profound impact on global health and the economy. Countries around the world have implemented measures to curb the spread of the virus, including lockdowns, social distancing, and vaccination campaigns. Despite these efforts, many challenges remain, including vaccine distribution, public compliance, and emerging variants.\n",
      "\n",
      "Generated Summary:\n",
      " The World Health Organization (WHO) has declared the CO-19 pandemic over.\n"
     ]
    }
   ],
   "source": [
    "# Display Summary\n",
    "print(\"Original Text:\\n\", input_text)\n",
    "print(\"\\nGenerated Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Function for Summarizing Multiple Texts\n",
    "def summarize_texts(texts, model, tokenizer, max_length=50):\n",
    "    summaries = []\n",
    "    for text in texts:\n",
    "        print(\"Processing text:\", text)\n",
    "        inputs = tokenizer(text, truncation=True, padding=\"longest\", max_length=512, return_tensors=\"pt\")\n",
    "        print(\"Tokenized inputs:\", inputs)\n",
    "        outputs = model.generate(inputs[\"input_ids\"], max_length=max_length, num_beams=5, early_stopping=True)\n",
    "        print(\"Generated outputs:\", outputs)\n",
    "        summaries.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        print(\"Decoded summary:\", summaries[-1])\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing multiple texts...\n",
      "Processing text: Artificial intelligence is transforming industries worldwide, with applications in healthcare, finance, and more.\n",
      "Tokenized inputs: {'input_ids': tensor([[16882,  3941,   117, 11204,  3217,  2586,   108,   122,  1160,   115,\n",
      "          2896,   108,  3324,   108,   111,   154,   107,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Generated outputs: tensor([[    0,   222,   150,   679,   113,  3439,   135,  2636,  8755,   108,\n",
      "         17569,   111, 22803,  6825,  8025,  9541,  9944,   978,   134,   109,\n",
      "           979,  4958,  3941,   117,   458,   124,   109, 10156,   107,     1]])\n",
      "Decoded summary: In our series of letters from African journalists, filmmaker and columnist Farai Sevenzo looks at the impact artificial intelligence is having on the continent.\n",
      "Processing text: Climate change is a pressing global issue, requiring immediate action to reduce greenhouse gas emissions.\n",
      "Tokenized inputs: {'input_ids': tensor([[ 9792,   411,   117,   114,  7596,  1122,   797,   108,  6279,  3321,\n",
      "           918,   112,  1329, 10282,  1503,  5662,   107,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Generated outputs: tensor([[    0,   398,   297,   113,   150,   679,   113,  3439,   135,  2636,\n",
      "          8755,   108, 17569,   111, 22803, 17964, 39374,   978,   134,   181,\n",
      "           113,   109,   662,   618,  2931,   109, 10156,   107,     1]])\n",
      "Decoded summary: As part of our series of letters from African journalists, filmmaker and columnist Ahmed Rashid looks at some of the key issues facing the continent.\n",
      "\n",
      "Text 1:\n",
      "Artificial intelligence is transforming industries worldwide, with applications in healthcare, finance, and more.\n",
      "\n",
      "Summary 1:\n",
      "In our series of letters from African journalists, filmmaker and columnist Farai Sevenzo looks at the impact artificial intelligence is having on the continent.\n",
      "\n",
      "Text 2:\n",
      "Climate change is a pressing global issue, requiring immediate action to reduce greenhouse gas emissions.\n",
      "\n",
      "Summary 2:\n",
      "As part of our series of letters from African journalists, filmmaker and columnist Ahmed Rashid looks at some of the key issues facing the continent.\n"
     ]
    }
   ],
   "source": [
    "# Example: Summarizing Multiple Texts\n",
    "texts = [\n",
    "    \"Artificial intelligence is transforming industries worldwide, with applications in healthcare, finance, and more.\",\n",
    "    \"Climate change is a pressing global issue, requiring immediate action to reduce greenhouse gas emissions.\",\n",
    "]\n",
    "\n",
    "print(\"Summarizing multiple texts...\")\n",
    "summaries = summarize_texts(texts, model, tokenizer)\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"\\nText {i+1}:\\n{texts[i]}\\n\\nSummary {i+1}:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next step, we can try with different model and evaluate which one we prefer to use as a summariser model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
