{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Question Answering Tutorial\n",
    "\n",
    "Since we are familiar with the use of LLM, I will quickly go through the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Libraries\n",
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select the `llm-env` kernel before running the notebook\n",
    "\n",
    "**Note**\n",
    "install jupyter dependencies if needed before using this notebook\n",
    "install torch based on their official website either in CPU or GPU based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained T5 Model for QA and Tokenizer\n",
    "model_name = \"t5-small\"  # Options: t5-small, t5-base, t5-large, t5-3b, t5-11b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PegasusTokenizer requires the SentencePiece library. We can install using:\n",
    "`python -m pip install sentencepiece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model for QA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vatha\\miniconda3\\envs\\llm-env\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vatha\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer and model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "print(\"Loading tokenizer and model for QA...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "print(\"Tokenizer and model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Context and Question\n",
    "context = (\n",
    "    \"The COVID-19 pandemic has had a profound impact on global health and the economy. \"\n",
    "    \"Countries around the world have implemented measures to curb the spread of the virus, \"\n",
    "    \"including lockdowns, social distancing, and vaccination campaigns. Despite these efforts, \"\n",
    "    \"many challenges remain, including vaccine distribution, public compliance, and emerging variants.\"\n",
    ")\n",
    "question = \"What measures have countries implemented to curb the spread of COVID-19?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text for QA: question: What measures have countries implemented to curb the spread of COVID-19? context: The COVID-19 pandemic has had a profound impact on global health and the economy. Countries around the world have implemented measures to curb the spread of the virus, including lockdowns, social distancing, and vaccination campaigns. Despite these efforts, many challenges remain, including vaccine distribution, public compliance, and emerging variants.\n"
     ]
    }
   ],
   "source": [
    "# Prepare Input for T5\n",
    "input_text = f\"question: {question} context: {context}\"  # T5 expects input in this format\n",
    "print(\"Input text for QA:\", input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing input for QA...\n",
      "Tokens for QA: {'input_ids': tensor([[  822,    10,   363,  3629,    43,  1440,  6960,    12, 16110,     8,\n",
      "          3060,    13,  2847,  7765,   308,  4481,    58,  2625,    10,    37,\n",
      "          2847,  7765,   308,  4481,  2131,   221,  3113,    65,   141,     3,\n",
      "             9, 13343,  1113,    30,  1252,   533,    11,     8,  2717,     5,\n",
      "         30763,   300,     8,   296,    43,  6960,  3629,    12, 16110,     8,\n",
      "          3060,    13,     8,  6722,     6,   379,  6081,  3035,     7,     6,\n",
      "           569,  1028,    17,     9,  4733,     6,    11, 24639,  8203,     5,\n",
      "             3,  4868,   175,  2231,     6,   186,  2428,  2367,     6,   379,\n",
      "         12956,  3438,     6,   452,  5856,     6,    11,  7821,  6826,     7,\n",
      "             5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize Input\n",
    "print(\"Tokenizing input for QA...\")\n",
    "tokens = tokenizer(input_text, truncation=True, padding=\"longest\", max_length=512, return_tensors=\"pt\")\n",
    "print(\"Tokens for QA:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer...\n",
      "Answer IDs: tensor([[    0,  6081,  3035,     7,     6,   569,  1028,    17,     9,  4733,\n",
      "             6,    11, 24639,  8203,     1]])\n"
     ]
    }
   ],
   "source": [
    "# Generate Answer\n",
    "print(\"Generating answer...\")\n",
    "answer_ids = model.generate(tokens[\"input_ids\"], max_length=50, num_beams=5, early_stopping=True)\n",
    "print(\"Answer IDs:\", answer_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded answer: lockdowns, social distancing, and vaccination campaigns\n"
     ]
    }
   ],
   "source": [
    "# Decode Answer\n",
    "answer = tokenizer.decode(answer_ids[0], skip_special_tokens=True)\n",
    "print(\"Decoded answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      " What measures have countries implemented to curb the spread of COVID-19?\n",
      "\n",
      "Context:\n",
      " The COVID-19 pandemic has had a profound impact on global health and the economy. Countries around the world have implemented measures to curb the spread of the virus, including lockdowns, social distancing, and vaccination campaigns. Despite these efforts, many challenges remain, including vaccine distribution, public compliance, and emerging variants.\n",
      "\n",
      "Generated Answer:\n",
      " lockdowns, social distancing, and vaccination campaigns\n"
     ]
    }
   ],
   "source": [
    "# Display QA Pair\n",
    "print(\"Question:\\n\", question)\n",
    "print(\"\\nContext:\\n\", context)\n",
    "print(\"\\nGenerated Answer:\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Function for Answering Multiple Questions\n",
    "def answer_questions(context, questions, model, tokenizer, max_length=50):\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        input_text = f\"question: {question} context: {context}\"\n",
    "        print(\"Processing question:\", question)\n",
    "        tokens = tokenizer(input_text, truncation=True, padding=\"longest\", max_length=512, return_tensors=\"pt\")\n",
    "        print(\"Tokenized input:\", tokens)\n",
    "        outputs = model.generate(tokens[\"input_ids\"], max_length=max_length, num_beams=5, early_stopping=True)\n",
    "        print(\"Generated outputs:\", outputs)\n",
    "        answers.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        print(\"Decoded answer:\", answers[-1])\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answering multiple questions...\n",
      "Processing question: What measures have countries implemented to curb the spread of COVID-19?\n",
      "Tokenized input: {'input_ids': tensor([[  822,    10,   363,  3629,    43,  1440,  6960,    12, 16110,     8,\n",
      "          3060,    13,  2847,  7765,   308,  4481,    58,  2625,    10,    37,\n",
      "          2847,  7765,   308,  4481,  2131,   221,  3113,    65,   141,     3,\n",
      "             9, 13343,  1113,    30,  1252,   533,    11,     8,  2717,     5,\n",
      "         30763,   300,     8,   296,    43,  6960,  3629,    12, 16110,     8,\n",
      "          3060,    13,     8,  6722,     6,   379,  6081,  3035,     7,     6,\n",
      "           569,  1028,    17,     9,  4733,     6,    11, 24639,  8203,     5,\n",
      "             3,  4868,   175,  2231,     6,   186,  2428,  2367,     6,   379,\n",
      "         12956,  3438,     6,   452,  5856,     6,    11,  7821,  6826,     7,\n",
      "             5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Generated outputs: tensor([[    0,  6081,  3035,     7,     6,   569,  1028,    17,     9,  4733,\n",
      "             6,    11, 24639,  8203,     1]])\n",
      "Decoded answer: lockdowns, social distancing, and vaccination campaigns\n",
      "Processing question: What challenges remain despite these efforts?\n",
      "Tokenized input: {'input_ids': tensor([[  822,    10,   363,  2428,  2367,     3,  3565,   175,  2231,    58,\n",
      "          2625,    10,    37,  2847,  7765,   308,  4481,  2131,   221,  3113,\n",
      "            65,   141,     3,     9, 13343,  1113,    30,  1252,   533,    11,\n",
      "             8,  2717,     5, 30763,   300,     8,   296,    43,  6960,  3629,\n",
      "            12, 16110,     8,  3060,    13,     8,  6722,     6,   379,  6081,\n",
      "          3035,     7,     6,   569,  1028,    17,     9,  4733,     6,    11,\n",
      "         24639,  8203,     5,     3,  4868,   175,  2231,     6,   186,  2428,\n",
      "          2367,     6,   379, 12956,  3438,     6,   452,  5856,     6,    11,\n",
      "          7821,  6826,     7,     5,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Generated outputs: tensor([[    0, 12956,  3438,     6,   452,  5856,     1]])\n",
      "Decoded answer: vaccine distribution, public compliance\n",
      "\n",
      "Question 1:\n",
      "What measures have countries implemented to curb the spread of COVID-19?\n",
      "\n",
      "Answer 1:\n",
      "lockdowns, social distancing, and vaccination campaigns\n",
      "\n",
      "Question 2:\n",
      "What challenges remain despite these efforts?\n",
      "\n",
      "Answer 2:\n",
      "vaccine distribution, public compliance\n"
     ]
    }
   ],
   "source": [
    "# Example: Answering Multiple Questions\n",
    "questions = [\n",
    "    \"What measures have countries implemented to curb the spread of COVID-19?\",\n",
    "    \"What challenges remain despite these efforts?\",\n",
    "]\n",
    "\n",
    "print(\"Answering multiple questions...\")\n",
    "answers = answer_questions(context, questions, model, tokenizer)\n",
    "for i, answer in enumerate(answers):\n",
    "    print(f\"\\nQuestion {i+1}:\\n{questions[i]}\\n\\nAnswer {i+1}:\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next step, we can try with different model and evaluate which one we prefer to use as a QA model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
